{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cd8af0e-9e22-4e08-9d6c-d09b813b1ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ./RWs_H_g_2p2_tadv_1min_Hs_g_1m.npz...\n",
      "Data loaded. Splitting training and validation data...\n",
      "Saving data to separate .npy files...\n",
      "\n",
      "Preprocessing complete. You can now run the 'train_on_datahub.py' script.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# SCRIPT 1: preprocess.py\n",
    "#\n",
    "# PURPOSE:\n",
    "# This script prepares the data for datahub\n",
    "# It loads the original, large '.npz' file (which contains ALL data) into RAM one time.\n",
    "# It then splits the data into training, validation, and testing sets.\n",
    "# Finally, it saves each of these sets as its *own* separate '.npy' file.\n",
    "# This allows our main 'train.py' script to load only the small batches it needs,\n",
    "# preventing the Out-of-Memory (OOM) errors.\n",
    "#\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# --- 1. DEFINE FILE PATHS ---\n",
    "# Define the directory where the data is stored.\n",
    "DATA_DIR = \"./\" \n",
    "\n",
    "# This is the INPUT file downloaded from the authors.\n",
    "npz_file = os.path.join(DATA_DIR, 'RWs_H_g_2p2_tadv_1min_Hs_g_1m.npz')\n",
    "\n",
    "# These are the OUTPUT files we are creating.\n",
    "x_train_file = os.path.join(DATA_DIR, 'x_train.npy')\n",
    "y_train_file = os.path.join(DATA_DIR, 'y_train.npy')\n",
    "x_val_file = os.path.join(DATA_DIR, 'x_val.npy')\n",
    "y_val_file = os.path.join(DATA_DIR, 'y_val.npy')\n",
    "x_test_file = os.path.join(DATA_DIR, 'x_test.npy')\n",
    "y_test_file = os.path.join(DATA_DIR, 'y_test.npy')\n",
    "\n",
    "# --- 2. DATA PREPROCESSING ---\n",
    "print(f\"Loading {npz_file}...\")\n",
    "try:\n",
    "    # Load the original .npz file into the computer's main RAM.\n",
    "    with np.load(npz_file) as data:\n",
    "        # The .npz file is like a zip file for arrays. We extract the ones we need.\n",
    "        # This 'wave_data_train' array is actually 80% of the total data.\n",
    "        wave_data_train_full = data['wave_data_train']\n",
    "        label_train_full = data['label_train']\n",
    "        # This is the final 20% test set.\n",
    "        wave_data_test = data['wave_data_test']\n",
    "        label_test = data['label_test']\n",
    "\n",
    "    print(\"Data loaded. Splitting training and validation data...\")\n",
    "    \n",
    "    # We split the 'wave_data_train_full' (80% of total) into a new, smaller\n",
    "    # training set (64%) and a validation set (16%).\n",
    "    # This matches the 64/16/20 split mentioned in the paper.\n",
    "    x_train, x_val, y_train, y_val = train_test_split(\n",
    "        wave_data_train_full,  # The 80% we loaded\n",
    "        label_train_full, \n",
    "        test_size=0.20,      # 20% of the 80% = 16% of the total\n",
    "        random_state=42      # Ensures the split is reproducible\n",
    "    )\n",
    "\n",
    "    print(\"Saving data to separate .npy files...\")\n",
    "    \n",
    "    # This is the key step: save each new array as its own file.\n",
    "    # The 'train.py' script will read from these.\n",
    "    np.save(x_train_file, x_train)\n",
    "    np.save(y_train_file, y_train)\n",
    "    np.save(x_val_file, x_val)\n",
    "    np.save(y_val_file, y_val)\n",
    "    np.save(x_test_file, wave_data_test)\n",
    "    np.save(y_test_file, label_test)\n",
    "    \n",
    "    print(\"\\nPreprocessing complete. Run the 'train_on_datahub.py' script.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Could not find '{npz_file}'.\")\n",
    "    print(\"Please make sure it is in the same directory.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24647b79-013c-4528-acaf-5d073fe4ccf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Data Generators...\n",
      "Data Generators created successfully.\n",
      "Model: \"Rogue_Wave_LSTM\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_4 (LSTM)               (None, 1536, 10)          480       \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 1536, 10)          40        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 1536, 10)          840       \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 1536, 10)          40        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (None, 1536, 10)          840       \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 1536, 10)          40        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 10)                840       \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 10)                40        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 10)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 22        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3182 (12.43 KB)\n",
      "Trainable params: 3102 (12.12 KB)\n",
      "Non-trainable params: 80 (320.00 Byte)\n",
      "_________________________________________________________________\n",
      "\n",
      "--- Starting Model Training (for up to 100 epochs) ---\n",
      "Epoch 1/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.7125 - accuracy: 0.5044\n",
      "Epoch 1: val_accuracy improved from -inf to 0.50402, saving model to ./best_rogue_wave_model.keras\n",
      "932/932 [==============================] - 214s 223ms/step - loss: 0.7125 - accuracy: 0.5044 - val_loss: 0.7078 - val_accuracy: 0.5040\n",
      "Epoch 2/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.6952 - accuracy: 0.5206\n",
      "Epoch 2: val_accuracy did not improve from 0.50402\n",
      "932/932 [==============================] - 205s 220ms/step - loss: 0.6952 - accuracy: 0.5206 - val_loss: 0.7326 - val_accuracy: 0.5035\n",
      "Epoch 3/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.6931 - accuracy: 0.5286\n",
      "Epoch 3: val_accuracy did not improve from 0.50402\n",
      "932/932 [==============================] - 211s 226ms/step - loss: 0.6931 - accuracy: 0.5286 - val_loss: 0.7046 - val_accuracy: 0.4920\n",
      "Epoch 4/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.6891 - accuracy: 0.5392\n",
      "Epoch 4: val_accuracy did not improve from 0.50402\n",
      "932/932 [==============================] - 205s 220ms/step - loss: 0.6891 - accuracy: 0.5392 - val_loss: 0.7004 - val_accuracy: 0.4997\n",
      "Epoch 5/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.6868 - accuracy: 0.5455\n",
      "Epoch 5: val_accuracy did not improve from 0.50402\n",
      "932/932 [==============================] - 212s 227ms/step - loss: 0.6868 - accuracy: 0.5455 - val_loss: 0.7126 - val_accuracy: 0.4946\n",
      "Epoch 6/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.6857 - accuracy: 0.5551\n",
      "Epoch 6: val_accuracy did not improve from 0.50402\n",
      "932/932 [==============================] - 208s 223ms/step - loss: 0.6857 - accuracy: 0.5551 - val_loss: 0.7047 - val_accuracy: 0.4981\n",
      "Epoch 7/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.6840 - accuracy: 0.5581\n",
      "Epoch 7: val_accuracy did not improve from 0.50402\n",
      "932/932 [==============================] - 212s 227ms/step - loss: 0.6840 - accuracy: 0.5581 - val_loss: 0.7236 - val_accuracy: 0.5027\n",
      "Epoch 8/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.6830 - accuracy: 0.5615\n",
      "Epoch 8: val_accuracy improved from 0.50402 to 0.51019, saving model to ./best_rogue_wave_model.keras\n",
      "932/932 [==============================] - 207s 222ms/step - loss: 0.6830 - accuracy: 0.5615 - val_loss: 0.6957 - val_accuracy: 0.5102\n",
      "Epoch 9/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.6844 - accuracy: 0.5557\n",
      "Epoch 9: val_accuracy improved from 0.51019 to 0.52200, saving model to ./best_rogue_wave_model.keras\n",
      "932/932 [==============================] - 205s 220ms/step - loss: 0.6844 - accuracy: 0.5557 - val_loss: 0.7061 - val_accuracy: 0.5220\n",
      "Epoch 10/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.6794 - accuracy: 0.5667\n",
      "Epoch 10: val_accuracy did not improve from 0.52200\n",
      "932/932 [==============================] - 209s 225ms/step - loss: 0.6794 - accuracy: 0.5667 - val_loss: 0.7390 - val_accuracy: 0.5003\n",
      "Epoch 11/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.6837 - accuracy: 0.5561\n",
      "Epoch 11: val_accuracy did not improve from 0.52200\n",
      "932/932 [==============================] - 205s 220ms/step - loss: 0.6837 - accuracy: 0.5561 - val_loss: 0.7187 - val_accuracy: 0.5035\n",
      "Epoch 12/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.6801 - accuracy: 0.5640\n",
      "Epoch 12: val_accuracy did not improve from 0.52200\n",
      "932/932 [==============================] - 211s 227ms/step - loss: 0.6801 - accuracy: 0.5640 - val_loss: 0.8201 - val_accuracy: 0.5000\n",
      "Epoch 13/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.6822 - accuracy: 0.5599\n",
      "Epoch 13: val_accuracy did not improve from 0.52200\n",
      "932/932 [==============================] - 206s 222ms/step - loss: 0.6822 - accuracy: 0.5599 - val_loss: 0.6966 - val_accuracy: 0.5016\n",
      "Epoch 14/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.6843 - accuracy: 0.5552\n",
      "Epoch 14: val_accuracy did not improve from 0.52200\n",
      "932/932 [==============================] - 209s 224ms/step - loss: 0.6843 - accuracy: 0.5552 - val_loss: 0.7652 - val_accuracy: 0.5005\n",
      "Epoch 15/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.6777 - accuracy: 0.5720\n",
      "Epoch 15: val_accuracy did not improve from 0.52200\n",
      "932/932 [==============================] - 209s 224ms/step - loss: 0.6777 - accuracy: 0.5720 - val_loss: 0.6931 - val_accuracy: 0.5115\n",
      "Epoch 16/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.6789 - accuracy: 0.5682\n",
      "Epoch 16: val_accuracy did not improve from 0.52200\n",
      "932/932 [==============================] - 207s 222ms/step - loss: 0.6789 - accuracy: 0.5682 - val_loss: 0.7147 - val_accuracy: 0.5164\n",
      "Epoch 17/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.6818 - accuracy: 0.5653\n",
      "Epoch 17: val_accuracy did not improve from 0.52200\n",
      "932/932 [==============================] - 209s 224ms/step - loss: 0.6818 - accuracy: 0.5653 - val_loss: 0.7244 - val_accuracy: 0.5089\n",
      "Epoch 18/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.6768 - accuracy: 0.5740\n",
      "Epoch 18: val_accuracy did not improve from 0.52200\n",
      "932/932 [==============================] - 212s 228ms/step - loss: 0.6768 - accuracy: 0.5740 - val_loss: 0.7470 - val_accuracy: 0.5000\n",
      "Epoch 19/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.6783 - accuracy: 0.5683\n",
      "Epoch 19: val_accuracy improved from 0.52200 to 0.55123, saving model to ./best_rogue_wave_model.keras\n",
      "932/932 [==============================] - 210s 225ms/step - loss: 0.6783 - accuracy: 0.5683 - val_loss: 0.6980 - val_accuracy: 0.5512\n",
      "Epoch 20/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.6771 - accuracy: 0.5736\n",
      "Epoch 20: val_accuracy did not improve from 0.55123\n",
      "932/932 [==============================] - 208s 223ms/step - loss: 0.6771 - accuracy: 0.5736 - val_loss: 0.7737 - val_accuracy: 0.4997\n",
      "Epoch 21/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.6775 - accuracy: 0.5726\n",
      "Epoch 21: val_accuracy did not improve from 0.55123\n",
      "932/932 [==============================] - 210s 225ms/step - loss: 0.6775 - accuracy: 0.5726 - val_loss: 0.7310 - val_accuracy: 0.5043\n",
      "Epoch 22/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.6759 - accuracy: 0.5747\n",
      "Epoch 22: val_accuracy did not improve from 0.55123\n",
      "932/932 [==============================] - 205s 220ms/step - loss: 0.6759 - accuracy: 0.5747 - val_loss: 0.7970 - val_accuracy: 0.5000\n",
      "Epoch 23/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.6759 - accuracy: 0.5763\n",
      "Epoch 23: val_accuracy did not improve from 0.55123\n",
      "932/932 [==============================] - 211s 226ms/step - loss: 0.6759 - accuracy: 0.5763 - val_loss: 0.8380 - val_accuracy: 0.5000\n",
      "Epoch 24/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.6728 - accuracy: 0.5795\n",
      "Epoch 24: val_accuracy did not improve from 0.55123\n",
      "932/932 [==============================] - 209s 224ms/step - loss: 0.6728 - accuracy: 0.5795 - val_loss: 0.7528 - val_accuracy: 0.5043\n",
      "Epoch 25/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.6723 - accuracy: 0.5867\n",
      "Epoch 25: val_accuracy improved from 0.55123 to 0.55714, saving model to ./best_rogue_wave_model.keras\n",
      "932/932 [==============================] - 211s 226ms/step - loss: 0.6723 - accuracy: 0.5867 - val_loss: 0.7017 - val_accuracy: 0.5571\n",
      "Epoch 26/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.6807 - accuracy: 0.5592\n",
      "Epoch 26: val_accuracy did not improve from 0.55714\n",
      "932/932 [==============================] - 210s 225ms/step - loss: 0.6807 - accuracy: 0.5592 - val_loss: 0.7369 - val_accuracy: 0.5048\n",
      "Epoch 27/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.6767 - accuracy: 0.5742\n",
      "Epoch 27: val_accuracy did not improve from 0.55714\n",
      "932/932 [==============================] - 211s 226ms/step - loss: 0.6767 - accuracy: 0.5742 - val_loss: 0.7136 - val_accuracy: 0.5011\n",
      "Epoch 28/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.6759 - accuracy: 0.5791\n",
      "Epoch 28: val_accuracy did not improve from 0.55714\n",
      "932/932 [==============================] - 210s 225ms/step - loss: 0.6759 - accuracy: 0.5791 - val_loss: 0.7211 - val_accuracy: 0.5046\n",
      "Epoch 29/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.6801 - accuracy: 0.5630\n",
      "Epoch 29: val_accuracy did not improve from 0.55714\n",
      "932/932 [==============================] - 210s 225ms/step - loss: 0.6801 - accuracy: 0.5630 - val_loss: 0.6947 - val_accuracy: 0.5091\n",
      "Epoch 30/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.6895 - accuracy: 0.5376\n",
      "Epoch 30: val_accuracy did not improve from 0.55714\n",
      "932/932 [==============================] - 212s 228ms/step - loss: 0.6895 - accuracy: 0.5376 - val_loss: 0.7048 - val_accuracy: 0.5024\n",
      "Epoch 31/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.6843 - accuracy: 0.5555\n",
      "Epoch 31: val_accuracy did not improve from 0.55714\n",
      "932/932 [==============================] - 204s 219ms/step - loss: 0.6843 - accuracy: 0.5555 - val_loss: 0.7168 - val_accuracy: 0.5121\n",
      "Epoch 32/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.6761 - accuracy: 0.5813\n",
      "Epoch 32: val_accuracy did not improve from 0.55714\n",
      "932/932 [==============================] - 202s 216ms/step - loss: 0.6761 - accuracy: 0.5813 - val_loss: 0.7130 - val_accuracy: 0.5000\n",
      "Epoch 33/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.6818 - accuracy: 0.5607\n",
      "Epoch 33: val_accuracy did not improve from 0.55714\n",
      "932/932 [==============================] - 199s 213ms/step - loss: 0.6818 - accuracy: 0.5607 - val_loss: 0.7282 - val_accuracy: 0.5110\n",
      "Epoch 34/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.6789 - accuracy: 0.5695\n",
      "Epoch 34: val_accuracy did not improve from 0.55714\n",
      "932/932 [==============================] - 211s 226ms/step - loss: 0.6789 - accuracy: 0.5695 - val_loss: 1.2248 - val_accuracy: 0.4995\n",
      "Epoch 35/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.6678 - accuracy: 0.5890\n",
      "Epoch 35: val_accuracy did not improve from 0.55714\n",
      "932/932 [==============================] - 212s 228ms/step - loss: 0.6678 - accuracy: 0.5890 - val_loss: 0.7550 - val_accuracy: 0.5051\n",
      "Epoch 36/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.6676 - accuracy: 0.5915\n",
      "Epoch 36: val_accuracy did not improve from 0.55714\n",
      "932/932 [==============================] - 212s 227ms/step - loss: 0.6676 - accuracy: 0.5915 - val_loss: 1.1143 - val_accuracy: 0.4995\n",
      "Epoch 37/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.6768 - accuracy: 0.5701\n",
      "Epoch 37: val_accuracy did not improve from 0.55714\n",
      "932/932 [==============================] - 211s 227ms/step - loss: 0.6768 - accuracy: 0.5701 - val_loss: 0.8090 - val_accuracy: 0.5008\n",
      "Epoch 38/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.6736 - accuracy: 0.5732\n",
      "Epoch 38: val_accuracy did not improve from 0.55714\n",
      "932/932 [==============================] - 208s 223ms/step - loss: 0.6736 - accuracy: 0.5732 - val_loss: 0.9182 - val_accuracy: 0.4995\n",
      "Epoch 39/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.6764 - accuracy: 0.5769\n",
      "Epoch 39: val_accuracy improved from 0.55714 to 0.56974, saving model to ./best_rogue_wave_model.keras\n",
      "932/932 [==============================] - 212s 228ms/step - loss: 0.6764 - accuracy: 0.5769 - val_loss: 0.6798 - val_accuracy: 0.5697\n",
      "Epoch 40/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.6779 - accuracy: 0.5683\n",
      "Epoch 40: val_accuracy did not improve from 0.56974\n",
      "932/932 [==============================] - 212s 228ms/step - loss: 0.6779 - accuracy: 0.5683 - val_loss: 0.6865 - val_accuracy: 0.5582\n",
      "Epoch 41/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.6802 - accuracy: 0.5670\n",
      "Epoch 41: val_accuracy did not improve from 0.56974\n",
      "932/932 [==============================] - 211s 226ms/step - loss: 0.6802 - accuracy: 0.5670 - val_loss: 0.6964 - val_accuracy: 0.5263\n",
      "Epoch 42/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.6776 - accuracy: 0.5699\n",
      "Epoch 42: val_accuracy did not improve from 0.56974\n",
      "932/932 [==============================] - 206s 221ms/step - loss: 0.6776 - accuracy: 0.5699 - val_loss: 0.7501 - val_accuracy: 0.4997\n",
      "Epoch 43/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.6740 - accuracy: 0.5771\n",
      "Epoch 43: val_accuracy did not improve from 0.56974\n",
      "932/932 [==============================] - 208s 223ms/step - loss: 0.6740 - accuracy: 0.5771 - val_loss: 0.7183 - val_accuracy: 0.5011\n",
      "Epoch 44/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.6707 - accuracy: 0.5828\n",
      "Epoch 44: val_accuracy did not improve from 0.56974\n",
      "932/932 [==============================] - 206s 221ms/step - loss: 0.6707 - accuracy: 0.5828 - val_loss: 0.9446 - val_accuracy: 0.4997\n",
      "Epoch 45/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.6693 - accuracy: 0.5905\n",
      "Epoch 45: val_accuracy did not improve from 0.56974\n",
      "932/932 [==============================] - 212s 228ms/step - loss: 0.6693 - accuracy: 0.5905 - val_loss: 0.8412 - val_accuracy: 0.5000\n",
      "Epoch 46/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.6590 - accuracy: 0.6030\n",
      "Epoch 46: val_accuracy did not improve from 0.56974\n",
      "932/932 [==============================] - 208s 223ms/step - loss: 0.6590 - accuracy: 0.6030 - val_loss: 1.1105 - val_accuracy: 0.4981\n",
      "Epoch 47/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.6025 - accuracy: 0.6723\n",
      "Epoch 47: val_accuracy did not improve from 0.56974\n",
      "932/932 [==============================] - 205s 220ms/step - loss: 0.6025 - accuracy: 0.6723 - val_loss: 1.4938 - val_accuracy: 0.4992\n",
      "Epoch 48/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.6234 - accuracy: 0.6455\n",
      "Epoch 48: val_accuracy did not improve from 0.56974\n",
      "932/932 [==============================] - 211s 227ms/step - loss: 0.6234 - accuracy: 0.6455 - val_loss: 2.4424 - val_accuracy: 0.5003\n",
      "Epoch 49/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.5659 - accuracy: 0.7064\n",
      "Epoch 49: val_accuracy improved from 0.56974 to 0.69769, saving model to ./best_rogue_wave_model.keras\n",
      "932/932 [==============================] - 214s 230ms/step - loss: 0.5659 - accuracy: 0.7064 - val_loss: 0.5946 - val_accuracy: 0.6977\n",
      "Epoch 50/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.5494 - accuracy: 0.7204\n",
      "Epoch 50: val_accuracy did not improve from 0.69769\n",
      "932/932 [==============================] - 204s 219ms/step - loss: 0.5494 - accuracy: 0.7204 - val_loss: 2.9002 - val_accuracy: 0.5008\n",
      "Epoch 51/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.5460 - accuracy: 0.7222\n",
      "Epoch 51: val_accuracy improved from 0.69769 to 0.73632, saving model to ./best_rogue_wave_model.keras\n",
      "932/932 [==============================] - 205s 220ms/step - loss: 0.5460 - accuracy: 0.7222 - val_loss: 0.5394 - val_accuracy: 0.7363\n",
      "Epoch 52/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.5426 - accuracy: 0.7276\n",
      "Epoch 52: val_accuracy did not improve from 0.73632\n",
      "932/932 [==============================] - 211s 226ms/step - loss: 0.5426 - accuracy: 0.7276 - val_loss: 3.0994 - val_accuracy: 0.4997\n",
      "Epoch 53/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.5360 - accuracy: 0.7330\n",
      "Epoch 53: val_accuracy did not improve from 0.73632\n",
      "932/932 [==============================] - 210s 225ms/step - loss: 0.5360 - accuracy: 0.7330 - val_loss: 3.2373 - val_accuracy: 0.5003\n",
      "Epoch 54/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.5358 - accuracy: 0.7314\n",
      "Epoch 54: val_accuracy did not improve from 0.73632\n",
      "932/932 [==============================] - 209s 224ms/step - loss: 0.5358 - accuracy: 0.7314 - val_loss: 1.1210 - val_accuracy: 0.5220\n",
      "Epoch 55/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.5339 - accuracy: 0.7372\n",
      "Epoch 55: val_accuracy did not improve from 0.73632\n",
      "932/932 [==============================] - 213s 229ms/step - loss: 0.5339 - accuracy: 0.7372 - val_loss: 4.0562 - val_accuracy: 0.4997\n",
      "Epoch 56/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.5332 - accuracy: 0.7341\n",
      "Epoch 56: val_accuracy did not improve from 0.73632\n",
      "932/932 [==============================] - 207s 222ms/step - loss: 0.5332 - accuracy: 0.7341 - val_loss: 3.1281 - val_accuracy: 0.5003\n",
      "Epoch 57/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.5338 - accuracy: 0.7359\n",
      "Epoch 57: val_accuracy did not improve from 0.73632\n",
      "932/932 [==============================] - 199s 214ms/step - loss: 0.5338 - accuracy: 0.7359 - val_loss: 1.5899 - val_accuracy: 0.5158\n",
      "Epoch 58/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.5320 - accuracy: 0.7353\n",
      "Epoch 58: val_accuracy did not improve from 0.73632\n",
      "932/932 [==============================] - 206s 221ms/step - loss: 0.5320 - accuracy: 0.7353 - val_loss: 3.7989 - val_accuracy: 0.5003\n",
      "Epoch 59/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.5316 - accuracy: 0.7336\n",
      "Epoch 59: val_accuracy did not improve from 0.73632\n",
      "932/932 [==============================] - 202s 217ms/step - loss: 0.5316 - accuracy: 0.7336 - val_loss: 1.8361 - val_accuracy: 0.5011\n",
      "Epoch 60/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.5289 - accuracy: 0.7365\n",
      "Epoch 60: val_accuracy did not improve from 0.73632\n",
      "932/932 [==============================] - 197s 212ms/step - loss: 0.5289 - accuracy: 0.7365 - val_loss: 4.6282 - val_accuracy: 0.5003\n",
      "Epoch 61/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.5308 - accuracy: 0.7376\n",
      "Epoch 61: val_accuracy did not improve from 0.73632\n",
      "932/932 [==============================] - 199s 214ms/step - loss: 0.5308 - accuracy: 0.7376 - val_loss: 2.1047 - val_accuracy: 0.5000\n",
      "Epoch 62/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.5313 - accuracy: 0.7341\n",
      "Epoch 62: val_accuracy did not improve from 0.73632\n",
      "932/932 [==============================] - 204s 219ms/step - loss: 0.5313 - accuracy: 0.7341 - val_loss: 2.8408 - val_accuracy: 0.4997\n",
      "Epoch 63/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.5267 - accuracy: 0.7373\n",
      "Epoch 63: val_accuracy did not improve from 0.73632\n",
      "932/932 [==============================] - 215s 231ms/step - loss: 0.5267 - accuracy: 0.7373 - val_loss: 0.6007 - val_accuracy: 0.6859\n",
      "Epoch 64/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.5251 - accuracy: 0.7416\n",
      "Epoch 64: val_accuracy did not improve from 0.73632\n",
      "932/932 [==============================] - 204s 219ms/step - loss: 0.5251 - accuracy: 0.7416 - val_loss: 2.0552 - val_accuracy: 0.4995\n",
      "Epoch 65/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.5201 - accuracy: 0.7426\n",
      "Epoch 65: val_accuracy did not improve from 0.73632\n",
      "932/932 [==============================] - 198s 213ms/step - loss: 0.5201 - accuracy: 0.7426 - val_loss: 4.8316 - val_accuracy: 0.5003\n",
      "Epoch 66/100\n",
      "932/932 [==============================] - ETA: 0s - loss: 0.5298 - accuracy: 0.7374\n",
      "Epoch 66: val_accuracy did not improve from 0.73632\n",
      "Restoring model weights from the end of the best epoch: 51.\n",
      "932/932 [==============================] - 206s 221ms/step - loss: 0.5298 - accuracy: 0.7374 - val_loss: 1.8461 - val_accuracy: 0.5126\n",
      "Epoch 66: early stopping\n",
      "\n",
      "--- Evaluating Best Model on Test Set ---\n",
      "\n",
      "Test Loss: 0.5357\n",
      "Test Accuracy: 0.7343\n",
      "\n",
      "Paper's reported accuracy was ~76% (0.7592).\n",
      "This result is very close to the paper's findings. Success!\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# SCRIPT 2: train_on_datahub.py\n",
    "# (Run this script after 'preprocess.py' to train the model)\n",
    "#\n",
    "# PURPOSE:\n",
    "# This script trains our LSTM model.\n",
    "# It uses a custom 'DataGenerator' class to stream data from the '.npy' files created in 'preprocess.py'. \n",
    "# This is the solution to the Datahub GPU memory limit. Instead of loading the whole dataset onto the GPU, this class feeds the model tiny batches (16 samples at a time).\n",
    "#\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, BatchNormalization, Dropout, Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# Callbacks are to make training smarter\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import os\n",
    "\n",
    "# --- 1. DEFINE FILE PATHS ---\n",
    "# These are the INPUT files (created by preprocess.py above)\n",
    "DATA_DIR = \"./\" \n",
    "x_train_file = os.path.join(DATA_DIR, 'x_train.npy')\n",
    "y_train_file = os.path.join(DATA_DIR, 'y_train.npy')\n",
    "x_val_file = os.path.join(DATA_DIR, 'x_val.npy')\n",
    "y_val_file = os.path.join(DATA_DIR, 'y_val.npy')\n",
    "x_test_file = os.path.join(DATA_DIR, 'x_test.npy')\n",
    "y_test_file = os.path.join(DATA_DIR, 'y_test.npy')\n",
    "\n",
    "# This is the OUTPUT file: the single best trained model\n",
    "best_model_file = os.path.join(DATA_DIR, 'best_rogue_wave_model.keras')\n",
    "\n",
    "# --- 2. KERAS DATA GENERATOR ---\n",
    "# This class is the core of solving the GPU issue.\n",
    "# It inherits from 'tf.keras.utils.Sequence', which is the standard\n",
    "# way to build a data pipeline for Keras.\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    # 'self' is the object, '__init__' is the \"constructor\" that sets it up\n",
    "    def __init__(self, x_file, y_file, batch_size):\n",
    "        # Load the .npy files into RAM. This is fine, Datahub has enough RAM.\n",
    "        # We are *not* loading them onto the tiny GPU.\n",
    "        self.x = np.load(x_file) \n",
    "        self.y = np.load(y_file)\n",
    "        self.batch_size = batch_size\n",
    "        self.num_samples = self.x.shape[0]\n",
    "        self.indices = np.arange(self.num_samples)\n",
    "        self.on_epoch_end() # Shuffle the data once at the start\n",
    "\n",
    "    def __len__(self):\n",
    "        # Keras needs to know how many batches are in one epoch.\n",
    "        # e.g., 14927 samples / 16 batch_size = 932 batches\n",
    "        return int(np.floor(self.num_samples / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # This is the function Keras calls to get a batch of data.\n",
    "        # 'index' is the batch number (e.g., batch #0, batch #1, ...)\n",
    "        \n",
    "        # Get the 16 indices for this specific batch\n",
    "        batch_indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        \n",
    "        # Get the data from RAM for just those 16 samples\n",
    "        x_batch = self.x[batch_indices]\n",
    "        y_batch = self.y[batch_indices]\n",
    "        \n",
    "        # Return the small batch, which Keras then sends to the GPU\n",
    "        return x_batch, y_batch\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        # This function is called by Keras at the end of every epoch\n",
    "        # to shuffle the data, so the model sees it in a different order.\n",
    "        np.random.shuffle(self.indices)\n",
    "\n",
    "# --- 3. MODEL TRAINING ---\n",
    "\n",
    "# Define Model Constants from the paper (Table 2)\n",
    "N_TIMESTEPS = 1536  # Each sample is 1536 data points long\n",
    "N_FEATURES = 1      # We only have one feature: wave height\n",
    "N_LSTM_UNITS = 10   # Number of neurons in each LSTM layer\n",
    "N_LSTM_LAYERS = 4   # Number of LSTM layers to stack\n",
    "DROPOUT_PROB = 0.05\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 16     # Small batch size for low GPU memory\n",
    "EPOCHS = 100        # Train for a long time. EarlyStopping will find the best one.\n",
    "\n",
    "try:\n",
    "    print(\"Initializing Data Generators...\")\n",
    "    # Create one generator for each data split\n",
    "    train_gen = DataGenerator(x_train_file, y_train_file, BATCH_SIZE)\n",
    "    val_gen = DataGenerator(x_val_file, y_val_file, BATCH_SIZE)\n",
    "    test_gen = DataGenerator(x_test_file, y_test_file, BATCH_SIZE)\n",
    "    print(\"Data Generators created successfully.\")\n",
    "\n",
    "    # --- Build the LSTM Model (from Figure 4) ---\n",
    "    model = Sequential(name=\"Rogue_Wave_LSTM\")\n",
    "    model.add(Input(shape=(N_TIMESTEPS, N_FEATURES)))\n",
    "    \n",
    "    # Loop to create the 4 stacked LSTM + BatchNormalization layers\n",
    "    for i in range(N_LSTM_LAYERS):\n",
    "        # 'return_sequences=True' is needed for all but the *last* LSTM layer\n",
    "        # so it can pass its full output sequence to the next layer.\n",
    "        is_last_layer = (i == N_LSTM_LAYERS - 1)\n",
    "        model.add(LSTM(N_LSTM_UNITS, return_sequences=not is_last_layer))\n",
    "        model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Dropout(DROPOUT_PROB))\n",
    "    \n",
    "    # Final output layer. 'Dense(2)' gives two outputs (Prob for class 0, Prob for class 1)\n",
    "    # 'softmax' ensures these two probabilities add up to 1.0.\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    model.summary() # Prints the model architecture\n",
    "    \n",
    "    # --- Compile the Model ---\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "        # 'sparse_categorical_crossentropy' is the correct loss function\n",
    "        # when labels are integers (0, 1) and the model has >1 output neuron.\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # --- 4. DEFINE CALLBACKS ---\n",
    "    \n",
    "    # This callback saves the model to a file *only* when the 'val_accuracy'\n",
    "    # improves. This way, we always have the \"best\" version saved.\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        filepath=best_model_file, # File to save to\n",
    "        monitor='val_accuracy',   # Metric to watch\n",
    "        save_best_only=True,      # Only save if it's the new best\n",
    "        mode='max',               # 'max' because we want *max* accuracy\n",
    "        verbose=1                 # Print a message when it saves\n",
    "    )\n",
    "    \n",
    "    # This callback stops the training early if the model isn't improving.\n",
    "    early_stopping_callback = EarlyStopping(\n",
    "        monitor='val_accuracy',     # Metric to watch\n",
    "        patience=15,              # Stop if no improvement for 15 epochs\n",
    "        mode='max',\n",
    "        verbose=1,\n",
    "        # This is the magic: it reloads the weights from the 'best_model_file'\n",
    "        # when it finishes. So, the 'model' object will be the best version.\n",
    "        restore_best_weights=True \n",
    "    )\n",
    "\n",
    "    # --- 5. TRAIN THE MODEL ---\n",
    "    print(\"\\n--- Starting Model Training (for up to 100 epochs) ---\")\n",
    "    \n",
    "    # We pass the 'generators' to model.fit, not the raw numpy arrays.\n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=val_gen,\n",
    "        workers=2, # Use 2 CPU cores to prepare data batches in parallel\n",
    "        callbacks=[checkpoint_callback, early_stopping_callback] \n",
    "    )\n",
    "    \n",
    "    # --- 6. EVALUATE THE BEST MODEL ON THE TEST SET ---\n",
    "    # Because of 'restore_best_weights=True', 'model' is already the best version.\n",
    "    \n",
    "    print(\"\\n--- Evaluating Best Model on Test Set ---\")\n",
    "    \n",
    "    # We use 'test_gen' (the final 20% of data) to get our final score.\n",
    "    # 'verbose=0' stops it from flooding the console with messages.\n",
    "    test_results = model.evaluate(test_gen, verbose=0) \n",
    "    \n",
    "    print(f\"\\nTest Loss: {test_results[0]:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_results[1]:.4f}\")\n",
    "    \n",
    "    print(f\"\\nPaper's reported accuracy was ~76% (0.7592).\")\n",
    "    if abs(test_results[1] - 0.7592) < 0.03:\n",
    "        print(\"Success!\")\n",
    "    else:\n",
    "        print(f\"Result: {test_results[1]*100:.2f}%\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: .npy files not found.\")\n",
    "    print(\"Run the 'preprocess' script first\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during training/evaluation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0686e5f-ac3c-47a3-8d1c-a321c909e90f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
